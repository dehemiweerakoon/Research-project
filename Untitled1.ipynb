{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4a61f4-dbd7-413e-9848-56d0e8367f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = [\n",
    "    # Programming Languages\n",
    "    \"Python\", \"Java\", \"C\", \"C++\", \"C#\", \"JavaScript\", \"TypeScript\", \"Go\", \"PHP\", \"Ruby\",\n",
    "    \"R\", \"Swift\", \"Kotlin\", \"Perl\", \"Shell Scripting\", \"Bash\", \"PowerShell\",\n",
    "\n",
    "    # Python Ecosystem\n",
    "    \"Django\", \"Flask\", \"FastAPI\", \"Pandas\", \"NumPy\", \"Scikit-learn\", \"TensorFlow\", \"PyTorch\",\n",
    "    \"Keras\", \"Matplotlib\", \"Seaborn\", \"NLTK\", \"OpenCV\", \"BeautifulSoup\", \"Celery\",\n",
    "\n",
    "    # Java Ecosystem\n",
    "    \"Spring\", \"Spring Boot\", \"Spring MVC\", \"Spring Security\", \"Spring Data JPA\", \"Hibernate\",\n",
    "    \"JPA\", \"JDBC\", \"Struts\", \"Maven\", \"Gradle\", \"JUnit\", \"Mockito\", \"JSP\", \"Servlets\",\n",
    "    \"Tomcat\", \"JBoss\", \"WildFly\", \"Jenkins\", \"Kafka\", \"RabbitMQ\",\n",
    "\n",
    "    # Web Development\n",
    "    \"HTML\", \"CSS\", \"SASS\", \"LESS\", \"Bootstrap\", \"Tailwind CSS\", \"JavaScript\", \"React\", \"Redux\",\n",
    "    \"Next.js\", \"Angular\", \"Vue.js\", \"Node.js\", \"Express\", \"jQuery\", \"AJAX\", \"REST\", \"RESTful APIs\",\n",
    "    \"GraphQL\", \"JSON\", \"XML\", \"WebSockets\", \"Responsive Design\", \"UI/UX\", \"Figma\", \"Adobe XD\",\n",
    "\n",
    "    # Databases\n",
    "    \"SQL\", \"MySQL\", \"PostgreSQL\", \"SQLite\", \"Oracle\", \"MongoDB\", \"Cassandra\", \"Redis\", \"Elasticsearch\",\n",
    "    \"MariaDB\", \"Firebase\", \"NoSQL\", \"PL/SQL\", \"Database Design\", \"ER Diagrams\", \"Stored Procedures\",\n",
    "    \"Indexing\", \"Query Optimization\", \"Normalization\", \"Backup\", \"Recovery\", \"Replication\",\n",
    "    \"Sharding\", \"Data Migration\", \"Triggers\", \"Transactions\", \"Access Control\",\n",
    "\n",
    "    # DevOps & Cloud\n",
    "    \"Docker\", \"Kubernetes\", \"CI/CD\", \"Git\", \"GitHub\", \"GitLab\", \"Bitbucket\", \"Terraform\",\n",
    "    \"Ansible\", \"Jenkins\", \"CircleCI\", \"AWS\", \"Azure\", \"Google Cloud\", \"CloudFormation\",\n",
    "    \"Serverless\", \"Lambda\", \"EC2\", \"S3\",\"Cisco networking\", \"CloudWatch\", \"ECS\", \"RDS\", \"VPC\", \"Cloud Security\",\n",
    "\n",
    "    # Networking & System Administration\n",
    "    \"Networking\", \"TCP/IP\", \"DNS\", \"DHCP\", \"LAN\", \"WAN\", \"VPN\", \"Load Balancing\", \"Routing\",\n",
    "    \"Switching\", \"Firewalls\", \"IDS\", \"IPS\", \"OSI Model\", \"Network Troubleshooting\",\n",
    "    \"Active Directory\", \"Windows Server\", \"Linux\", \"Ubuntu\", \"CentOS\", \"Red Hat\",\n",
    "    \"Shell Scripting\", \"System Monitoring\", \"Virtualization\", \"VMware\", \"Hyper-V\",\n",
    "    \"Patch Management\", \"Automation\", \"Nagios\", \"Zabbix\", \"Backup and Recovery\",\n",
    "\n",
    "    # Security & Analysis\n",
    "    \"Cybersecurity\", \"Network Security\", \"Application Security\", \"Vulnerability Assessment\",\n",
    "    \"Penetration Testing\", \"Incident Response\", \"Threat Analysis\", \"Risk Assessment\",\n",
    "    \"Ethical Hacking\", \"Kali Linux\", \"Wireshark\", \"Nmap\", \"Burp Suite\", \"Metasploit\",\n",
    "    \"SIEM\", \"Splunk\", \"Security Policies\", \"Encryption\", \"Authentication\", \"Access Control\",\n",
    "    \"Identity Management\", \"SOC\", \"FireEye\", \"Forensics\", \"Compliance\", \"GDPR\", \"ISO 27001\",\n",
    "\n",
    "    # Software Development Practices\n",
    "    \"Agile\", \"Scrum\", \"Kanban\", \"Waterfall\", \"SDLC\", \"Version Control\", \"Code Review\",\n",
    "    \"Unit Testing\", \"Integration Testing\", \"Functional Testing\", \"Regression Testing\",\n",
    "    \"Performance Testing\", \"TDD\", \"BDD\", \"Design Patterns\", \"Clean Code\", \"OOP\",\n",
    "    \"Functional Programming\", \"Multithreading\", \"Microservices\", \"API Development\",\n",
    "    \"API Integration\", \"Dependency Injection\", \"SOLID Principles\", \"Refactoring\",\n",
    "\n",
    "    # Project Management & Tools\n",
    "    \"Project Management\", \"Risk Management\", \"Resource Allocation\", \"Stakeholder Management\",\n",
    "    \"Change Management\", \"Scheduling\", \"Budgeting\", \"Leadership\", \"Communication\",\n",
    "    \"Team Management\", \"Collaboration\", \"JIRA\", \"Confluence\", \"Trello\", \"Microsoft Project\",\n",
    "    \"Asana\", \"Notion\", \"Slack\",\n",
    "\n",
    "    # Data & Analytics\n",
    "    \"Data Analysis\", \"Data Visualization\", \"Excel\", \"Power BI\", \"Tableau\", \"Google Data Studio\",\n",
    "    \"ETL\", \"Big Data\", \"Apache Spark\", \"Hadoop\", \"Hive\", \"Pig\", \"Data Warehousing\",\n",
    "    \"Machine Learning\", \"Deep Learning\", \"AI\", \"Natural Language Processing\", \"Model Deployment\",\n",
    "    \"Feature Engineering\", \"Hyperparameter Tuning\",\n",
    "\n",
    "    # Web Servers & APIs\n",
    "    \"Nginx\", \"Apache\", \"IIS\", \"Reverse Proxy\", \"Load Balancer\", \"SSL\", \"HTTPS\", \"OAuth2\",\n",
    "    \"JWT\", \"API Gateway\", \"Swagger\", \"Postman\", \"API Documentation\",\n",
    "\n",
    "    # Software Tools\n",
    "    \"VS Code\", \"IntelliJ IDEA\", \"Eclipse\", \"PyCharm\", \"NetBeans\", \"Postman\", \"Docker Compose\",\n",
    "    \"PgAdmin\", \"MySQL Workbench\", \"HeidiSQL\", \"DBeaver\", \"Wireshark\", \"Burp Suite\",\n",
    "\n",
    "    # Operating Systems\n",
    "    \"Windows\", \"Linux\", \"macOS\", \"Ubuntu\", \"CentOS\", \"Debian\", \"Kali Linux\",\n",
    "\n",
    "    # Other Technical Skills\n",
    "    \"SEO\", \"Content Delivery Networks\", \"Caching\", \"Session Management\", \"Responsive Design\",\n",
    "    \"Accessibility\", \"Performance Optimization\", \"Cloud Security\", \"Server Administration\",\n",
    "    \"Monitoring\", \"Logging\", \"Continuous Deployment\", \"Containerization\", \"Infrastructure as Code\",\n",
    "\n",
    "    # Soft Skills\n",
    "    \"Problem Solving\", \"Analytical Thinking\", \"Teamwork\", \"Communication\", \"Adaptability\",\n",
    "    \"Time Management\", \"Critical Thinking\", \"Attention to Detail\", \"Decision Making\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16fa9dbe-4940-4de7-a35e-68ac4eac3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7080f6d0-93ee-4aff-913d-e057c9fb8614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\users\\dehem\\anaconda3\\envs\\projectdata\\lib\\site-packages (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22fdd87-b923-49b7-be8b-a70a03861780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b95037-6593-4c03-adcc-67c37cae84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Category                                             Resume\n",
      "0  Frontend Developer  As a seasoned Frontend Developer, I have a pro...\n",
      "1   Backend Developer  With a solid background in Backend Development...\n",
      "2    Python Developer  As a Python Developer, I leverage my expertise...\n",
      "3      Data Scientist  With a background in Data Science, I possess a...\n",
      "4  Frontend Developer  Experienced Frontend Developer with a passion ...\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('data 02.csv',encoding='utf-8')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265192d9-3db2-44f7-8e69-fdd29ee1b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_skills(text, skills):\n",
    "    entities = []\n",
    "    for skill in skills:\n",
    "        for match in re.finditer(r'\\b' + re.escape(skill) + r'\\b', text, flags=re.IGNORECASE):\n",
    "            entities.append((match.start(), match.end(), 'SKILL'))\n",
    "    return (text, {'entities': entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39adb48-aa03-4a33-962a-489a661f0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_DATA = [annotate_skills(text, skills) for text in df['Resume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59ad8d8-b2fc-4d43-987c-c62dffb254d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe42183b-7799-4194-81b9-fecde89ad00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_skill_spans(text, skills):\n",
    "    i = 1\n",
    "    nlp = spacy.blank(\"en\")  # tokenizer\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for skill in sorted(skills, key=len, reverse=True):\n",
    "        pattern = r'(?<!\\w)' + re.escape(skill) + r'(?!\\w)'\n",
    "        for match in re.finditer(pattern, text, flags=re.IGNORECASE):\n",
    "            start, end = match.span()\n",
    "            # Adjust to token boundaries\n",
    "            token_start = doc.char_span(start, end, alignment_mode=\"expand\")\n",
    "            if token_start:\n",
    "                entities.append((token_start.start_char, token_start.end_char, \"SKILL\"))\n",
    "    print(text)\n",
    "    return (text, {\"entities\": entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5564fce1-189e-4e9d-bbf7-509cf1ec4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "\n",
    "def clean_skill_spans(text, skills):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "\n",
    "    for skill in sorted(skills, key=len, reverse=True):\n",
    "        pattern = r'(?<!\\w)' + re.escape(skill) + r'(?!\\w)'\n",
    "        for match in re.finditer(pattern, text, flags=re.IGNORECASE):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end, label=\"SKILL\", alignment_mode=\"expand\")\n",
    "            if span:\n",
    "                # skip if overlapping\n",
    "                if any(s <= span.start_char < e or s < span.end_char <= e for s, e, _ in entities):\n",
    "                    continue\n",
    "                entities.append((span.start_char, span.end_char, \"SKILL\"))\n",
    "\n",
    "    return (text, {\"entities\": entities})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcecbc4c-5803-4bc8-8393-d1d1af2494cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [clean_skill_spans(text, skills) for text in df['Resume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88900045-05c3-46a5-af1d-7d5c287321b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d7dd89-fa02-4e34-97de-7baa191e2348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 519.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training data saved to train.spacy\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "db = DocBin()\n",
    "\n",
    "for text, annotations in tqdm(TRAIN_DATA):\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "\n",
    "    for start, end, label in annotations[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span is None:\n",
    "            print(f\"⚠️ Misaligned span skipped: {text[start:end]}\")\n",
    "            continue\n",
    "   ch     # avoid overlapping spans\n",
    "        if any(span.start < e.end and span.end > e.start for e in ents):\n",
    "            print(f\"⚠️ Overlapping span skipped: {text[start:end]}\")\n",
    "            continue\n",
    "        ents.append(span)\n",
    "\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"train.spacy\")\n",
    "print(\"✅ Training data saved to train.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f933cb-a703-4106-b5bb-440057025c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': np.float32(16217.497)}\n",
      "Iteration 2, Losses: {'ner': np.float32(5551.976)}\n",
      "Iteration 3, Losses: {'ner': np.float32(1937.3107)}\n",
      "Iteration 4, Losses: {'ner': np.float32(791.9452)}\n",
      "Iteration 5, Losses: {'ner': np.float32(333.1563)}\n",
      "Iteration 6, Losses: {'ner': np.float32(163.76672)}\n",
      "Iteration 7, Losses: {'ner': np.float32(154.77008)}\n",
      "Iteration 8, Losses: {'ner': np.float32(161.7148)}\n",
      "Iteration 9, Losses: {'ner': np.float32(206.81229)}\n",
      "Iteration 10, Losses: {'ner': np.float32(85.30571)}\n",
      "Iteration 11, Losses: {'ner': np.float32(85.299065)}\n",
      "Iteration 12, Losses: {'ner': np.float32(57.627117)}\n",
      "Iteration 13, Losses: {'ner': np.float32(23.887342)}\n",
      "Iteration 14, Losses: {'ner': np.float32(21.7535)}\n",
      "Iteration 15, Losses: {'ner': np.float32(19.499908)}\n",
      "Iteration 16, Losses: {'ner': np.float32(10.46283)}\n",
      "Iteration 17, Losses: {'ner': np.float32(17.53307)}\n",
      "Iteration 18, Losses: {'ner': np.float32(6.6873302)}\n",
      "Iteration 19, Losses: {'ner': np.float32(40.982716)}\n",
      "Iteration 20, Losses: {'ner': np.float32(33.991856)}\n",
      "Iteration 21, Losses: {'ner': np.float32(20.092133)}\n",
      "Iteration 22, Losses: {'ner': np.float32(14.6540165)}\n",
      "Iteration 23, Losses: {'ner': np.float32(14.863681)}\n",
      "Iteration 24, Losses: {'ner': np.float32(19.560253)}\n",
      "Iteration 25, Losses: {'ner': np.float32(12.155303)}\n",
      "Iteration 26, Losses: {'ner': np.float32(9.339707)}\n",
      "Iteration 27, Losses: {'ner': np.float32(4.1985545)}\n",
      "Iteration 28, Losses: {'ner': np.float32(5.748671)}\n",
      "Iteration 29, Losses: {'ner': np.float32(6.5970516)}\n",
      "Iteration 30, Losses: {'ner': np.float32(11.251105)}\n",
      "✅ NER model trained and saved to 'skill_ner_model'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Load blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add NER pipeline if not exists\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Load training data from train.spacy\n",
    "doc_bin = DocBin().from_disk(\"train.spacy\")\n",
    "train_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "# Add SKILL label to NER\n",
    "ner.add_label(\"SKILL\")\n",
    "\n",
    "# Begin training\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(30):  # number of iterations\n",
    "    losses = {}\n",
    "    # create minibatches\n",
    "    batches = minibatch(train_docs, size=compounding(4.0, 32.0, 1.5))\n",
    "    for batch in batches:\n",
    "        examples = [Example(doc, doc) for doc in batch]\n",
    "        nlp.update(examples, drop=0.2, sgd=optimizer, losses=losses)\n",
    "    print(f\"Iteration {i+1}, Losses: {losses}\")\n",
    "\n",
    "# Save trained model\n",
    "nlp.to_disk(\"skill_ner_model\")\n",
    "print(\"✅ NER model trained and saved to 'skill_ner_model'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c2d1d8-7a0a-4c2a-a3ad-720fe2266701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas → SKILL\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "nlp_test = spacy.load(\"skill_ner_model\")\n",
    "\n",
    "# Test example\n",
    "doc = nlp_test(\"pandas\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"→\", ent.label_)\n",
    "# only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6926d0d3-1298-455f-9dd5-2e925d29727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%javascript\n"
     ]
    }
   ],
   "source": [
    "print(\"%%javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71eb52b-dc5b-4809-a42b-ab2953713e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46a98a-9c25-4b58-b262-0435f53cd57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
